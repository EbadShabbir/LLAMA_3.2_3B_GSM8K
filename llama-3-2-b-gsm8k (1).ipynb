{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install dependencies\n!pip install transformers datasets torch nltk rouge_score psutil gpustat\n\n# Set environment variable to reduce memory fragmentation\nimport os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\n# Hugging Face login\nfrom huggingface_hub import login\nlogin(\"HUUGING FACE CODE\")  # Replace with your actual Hugging Face token\n\nimport time\nimport torch\nimport psutil\nimport gpustat\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom datasets import load_dataset\nfrom nltk.translate.bleu_score import sentence_bleu\nfrom rouge_score import rouge_scorer\nimport numpy as np\nfrom tqdm import tqdm\n\n# Load model and tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"meta-llama/Llama-3.2-3B\",\n    device_map=\"auto\",  # Use \"0\" for single GPU if OOM persists\n    torch_dtype=torch.float16\n)\nmodel.eval()\n\n# Load GSM8K dataset (first 100 samples)\ndataset = load_dataset(\"gsm8k\", \"main\")[\"train\"].select(range(100))\n\n# Initialize metrics\nlatencies, tokens_per_sec, perplexities, bleus, rouge1s, rougeLs, memories, f1s = [], [], [], [], [], [], [], []\nscorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n\n# Evaluation loop\nfor sample in tqdm(dataset, desc=\"Evaluating\"):\n    question = sample[\"question\"]\n    reference = sample[\"answer\"].split(\"#### \")[-1].strip()\n    prompt = f\"Solve this math problem: {question}\\nProvide the final answer as a number or concise phrase.\"\n\n    # Tokenize input\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n    input_length = inputs.input_ids.size(1)\n\n    # Clear GPU memory\n    torch.cuda.empty_cache()\n\n    # Measure latency and generate\n    start_time = time.time()\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=20,  # Reduced to minimize memory\n            do_sample=False\n        )\n    latency = time.time() - start_time\n\n    # Decode output\n    generated = tokenizer.decode(outputs[0][input_length:], skip_special_tokens=True).strip()\n    output_length = outputs[0][input_length:].size(0)\n\n    # Latency and tokens per second\n    latencies.append(latency)\n    tokens_per_sec.append(output_length / latency if latency > 0 else 0)\n\n    # Perplexity (forward pass, skip for very short sequences)\n    if output_length > 2:\n        with torch.no_grad():\n            input_ids = outputs[:, input_length:].to(\"cuda\")\n            labels = input_ids.clone().to(\"cuda\")\n            outputs_forward = model(input_ids, labels=labels)\n            perplexity = torch.exp(outputs_forward.loss).item()\n        perplexities.append(perplexity)\n    else:\n        perplexities.append(float('inf'))\n\n    # BLEU and ROUGE\n    bleu = sentence_bleu([reference.split()], generated.split())\n    bleus.append(bleu)\n    rouge_scores = scorer.score(reference, generated)\n    rouge1s.append(rouge_scores['rouge1'].fmeasure)\n    rougeLs.append(rouge_scores['rougeL'].fmeasure)\n\n    # Memory usage (GPU)\n    gpu_stats = gpustat.new_query().gpus[0]\n    memory_used = gpu_stats.memory_used / 1024  # Convert MB to GB\n    memories.append(memory_used)\n\n    # F1 Score (binary: correct or not)\n    is_correct = generated == reference\n    f1 = 1.0 if is_correct else 0.0\n    f1s.append(f1)\n\n    # Log memory usage for debugging\n    print(f\"Sample {len(latencies)}: GPU Memory Used: {memory_used:.3f} GB\")\n\n# Compute averages\navg_latency = np.mean(latencies)\navg_tps = np.mean(tokens_per_sec)\navg_perplexity = np.mean([p for p in perplexities if p != float('inf')]) if perplexities else float('inf')\navg_bleu = np.mean(bleus)\navg_rouge1 = np.mean(rouge1s)\navg_rougeL = np.mean(rougeLs)\navg_memory = np.mean(memories)\navg_f1 = np.mean(f1s)\navg_knowledge_retention = avg_f1\navg_flop_reduction = 0.0\navg_retrieval_latency = 0.0\navg_memory_reduction = 0.0\navg_query_time = avg_latency\navg_accuracy_drop = 0.0\navg_compression_ratio = 1.0\n\n# Print results\nprint(f\"Avg latency: {avg_latency:.3f} sec\")\nprint(f\"Tokens per sec: {avg_tps:.2f}\")\nprint(f\"Avg perplexity: {avg_perplexity:.2f}\")\nprint(f\"BLEU Score: {avg_bleu:.3f}\")\nprint(f\"ROUGE-1 Score: {avg_rouge1:.3f}\")\nprint(f\"ROUGE-L Score: {avg_rougeL:.3f}\")\nprint(f\"Memory usage (GB): {avg_memory:.3f}\")\nprint(f\"FLOP Reduction (%): {avg_flop_reduction:.2f}\")\nprint(f\"Retrieval Latency (sec): {avg_retrieval_latency:.3f}\")\nprint(f\"F1 Score: {avg_f1:.3f}\")\nprint(f\"Knowledge Retention: {avg_knowledge_retention:.3f}\")\nprint(f\"Memory Reduction (%): {avg_memory_reduction:.2f}\")\nprint(f\"Query Processing Time (sec): {avg_query_time:.3f}\")\nprint(f\"Accuracy Drop: {avg_accuracy_drop:.3f}\")\nprint(f\"Compression Ratio: {avg_compression_ratio:.2f}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-21T18:03:22.850585Z","iopub.execute_input":"2025-04-21T18:03:22.851200Z","iopub.status.idle":"2025-04-21T18:05:13.507723Z","shell.execute_reply.started":"2025-04-21T18:03:22.851173Z","shell.execute_reply":"2025-04-21T18:05:13.506901Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\nRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\nRequirement already satisfied: rouge_score in /usr/local/lib/python3.11/dist-packages (0.1.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (7.0.0)\nRequirement already satisfied: gpustat in /usr/local/lib/python3.11/dist-packages (1.1.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.16)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\nRequirement already satisfied: nvidia-ml-py>=11.450.129 in /usr/local/lib/python3.11/dist-packages (from gpustat) (12.570.86)\nRequirement already satisfied: blessed>=1.17.1 in /usr/local/lib/python3.11/dist-packages (from gpustat) (1.20.0)\nRequirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.11/dist-packages (from blessed>=1.17.1->gpustat) (0.2.13)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71750a3aee28450bb26fe044525c89f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd48962196c44d4292195f6590c18d93"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fca8dd345cc45ec81ab0c18cd8482b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/844 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb1c3725d1314720b9f2a8e69e7e3f85"}},"metadata":{}},{"name":"stderr","text":"2025-04-21 18:03:45.483793: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745258625.756047      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745258625.834301      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab05341a93534bfeb9a4e32b4ae93116"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce968861d112430789207e68ad4f1f7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be30db2dc3e3443ca285ff5dff45ea5d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aae220b1131b471092bcdb079e49ea46"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53808ff689ff4b55b5311fd38c915c5c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddee8f35c70f499e889983f765ee9bc8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/7.94k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80cb8d059698417cb95e90180179f4f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/2.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e108bd82f698407582f9fd3fd54e720a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/419k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1677f178df82483a933202f56e8ff084"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/7473 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c145b68aca8f4acea2ac10167def085c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1319 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36ecd962aa4f4009b886f543846aa47b"}},"metadata":{}},{"name":"stderr","text":"Evaluating:   0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n  warnings.warn(\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nEvaluating:   1%|          | 1/100 [00:01<01:41,  1.02s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 1: GPU Memory Used: 3.143 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:   2%|▏         | 2/100 [00:01<00:56,  1.72it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 2: GPU Memory Used: 3.143 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:   3%|▎         | 3/100 [00:01<00:43,  2.23it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 3: GPU Memory Used: 3.143 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:   4%|▍         | 4/100 [00:01<00:36,  2.61it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 4: GPU Memory Used: 3.143 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:   5%|▌         | 5/100 [00:03<01:07,  1.40it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 5: GPU Memory Used: 3.160 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:   6%|▌         | 6/100 [00:03<00:53,  1.76it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 6: GPU Memory Used: 3.145 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:   7%|▋         | 7/100 [00:03<00:44,  2.09it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 7: GPU Memory Used: 3.145 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:   8%|▊         | 8/100 [00:04<01:05,  1.40it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 8: GPU Memory Used: 3.170 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:   9%|▉         | 9/100 [00:05<00:53,  1.72it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 9: GPU Memory Used: 3.148 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  10%|█         | 10/100 [00:06<01:09,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 10: GPU Memory Used: 3.164 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  11%|█         | 11/100 [00:06<00:55,  1.60it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 11: GPU Memory Used: 3.146 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  12%|█▏        | 12/100 [00:07<00:46,  1.91it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 12: GPU Memory Used: 3.148 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  13%|█▎        | 13/100 [00:07<00:38,  2.24it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 13: GPU Memory Used: 3.145 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  14%|█▍        | 14/100 [00:07<00:34,  2.52it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 14: GPU Memory Used: 3.146 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  15%|█▌        | 15/100 [00:08<00:53,  1.58it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 15: GPU Memory Used: 3.162 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  16%|█▌        | 16/100 [00:10<01:08,  1.23it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 16: GPU Memory Used: 3.166 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  17%|█▋        | 17/100 [00:10<00:54,  1.53it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 17: GPU Memory Used: 3.145 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  18%|█▊        | 18/100 [00:10<00:44,  1.85it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 18: GPU Memory Used: 3.146 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  19%|█▉        | 19/100 [00:10<00:37,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 19: GPU Memory Used: 3.146 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  20%|██        | 20/100 [00:12<00:55,  1.45it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 20: GPU Memory Used: 3.164 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  21%|██        | 21/100 [00:12<00:45,  1.75it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 21: GPU Memory Used: 3.146 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  22%|██▏       | 22/100 [00:12<00:37,  2.07it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 22: GPU Memory Used: 3.145 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  23%|██▎       | 23/100 [00:12<00:32,  2.36it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 23: GPU Memory Used: 3.145 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  24%|██▍       | 24/100 [00:13<00:29,  2.61it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 24: GPU Memory Used: 3.145 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  25%|██▌       | 25/100 [00:13<00:26,  2.83it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 25: GPU Memory Used: 3.146 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  26%|██▌       | 26/100 [00:13<00:24,  2.99it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 26: GPU Memory Used: 3.148 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  27%|██▋       | 27/100 [00:14<00:23,  3.12it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 27: GPU Memory Used: 3.145 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  28%|██▊       | 28/100 [00:14<00:22,  3.21it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 28: GPU Memory Used: 3.148 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  29%|██▉       | 29/100 [00:14<00:21,  3.29it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 29: GPU Memory Used: 3.145 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  30%|███       | 30/100 [00:14<00:20,  3.34it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 30: GPU Memory Used: 3.145 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  31%|███       | 31/100 [00:15<00:20,  3.39it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 31: GPU Memory Used: 3.145 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  32%|███▏      | 32/100 [00:15<00:19,  3.42it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 32: GPU Memory Used: 3.146 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  33%|███▎      | 33/100 [00:15<00:19,  3.35it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 33: GPU Memory Used: 3.150 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  34%|███▍      | 34/100 [00:16<00:19,  3.38it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 34: GPU Memory Used: 3.146 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  35%|███▌      | 35/100 [00:16<00:18,  3.46it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 35: GPU Memory Used: 3.145 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  36%|███▌      | 36/100 [00:16<00:18,  3.48it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 36: GPU Memory Used: 3.145 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  37%|███▋      | 37/100 [00:16<00:17,  3.54it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 37: GPU Memory Used: 3.146 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  38%|███▊      | 38/100 [00:17<00:17,  3.52it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 38: GPU Memory Used: 3.146 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  39%|███▉      | 39/100 [00:17<00:17,  3.51it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 39: GPU Memory Used: 3.146 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  40%|████      | 40/100 [00:17<00:17,  3.49it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 40: GPU Memory Used: 3.146 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  41%|████      | 41/100 [00:18<00:16,  3.56it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 41: GPU Memory Used: 3.145 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  42%|████▏     | 42/100 [00:18<00:16,  3.53it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 42: GPU Memory Used: 3.146 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  43%|████▎     | 43/100 [00:18<00:16,  3.52it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 43: GPU Memory Used: 3.146 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  44%|████▍     | 44/100 [00:18<00:15,  3.51it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 44: GPU Memory Used: 3.145 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  45%|████▌     | 45/100 [00:19<00:15,  3.51it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 45: GPU Memory Used: 3.146 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  46%|████▌     | 46/100 [00:19<00:15,  3.49it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 46: GPU Memory Used: 3.148 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  47%|████▋     | 47/100 [00:19<00:14,  3.55it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 47: GPU Memory Used: 3.145 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  48%|████▊     | 48/100 [00:20<00:14,  3.54it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 48: GPU Memory Used: 3.145 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  49%|████▉     | 49/100 [00:20<00:14,  3.52it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 49: GPU Memory Used: 3.145 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  50%|█████     | 50/100 [00:20<00:14,  3.52it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 50: GPU Memory Used: 3.145 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  51%|█████     | 51/100 [00:20<00:14,  3.50it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 51: GPU Memory Used: 3.146 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  52%|█████▏    | 52/100 [00:21<00:13,  3.48it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 52: GPU Memory Used: 3.146 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  53%|█████▎    | 53/100 [00:21<00:13,  3.47it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 53: GPU Memory Used: 3.145 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  54%|█████▍    | 54/100 [00:21<00:13,  3.48it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 54: GPU Memory Used: 3.145 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  55%|█████▌    | 55/100 [00:22<00:12,  3.48it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 55: GPU Memory Used: 3.146 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  56%|█████▌    | 56/100 [00:22<00:12,  3.54it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 56: GPU Memory Used: 3.145 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  57%|█████▋    | 57/100 [00:22<00:12,  3.53it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 57: GPU Memory Used: 3.145 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  58%|█████▊    | 58/100 [00:22<00:11,  3.50it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 58: GPU Memory Used: 3.145 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  59%|█████▉    | 59/100 [00:23<00:11,  3.49it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 59: GPU Memory Used: 3.146 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  60%|██████    | 60/100 [00:23<00:11,  3.46it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 60: GPU Memory Used: 3.148 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  61%|██████    | 61/100 [00:23<00:11,  3.47it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 61: GPU Memory Used: 3.145 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  62%|██████▏   | 62/100 [00:24<00:11,  3.45it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 62: GPU Memory Used: 3.150 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  63%|██████▎   | 63/100 [00:24<00:10,  3.53it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 63: GPU Memory Used: 3.145 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  64%|██████▍   | 64/100 [00:24<00:10,  3.51it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 64: GPU Memory Used: 3.148 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  65%|██████▌   | 65/100 [00:25<00:16,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 65: GPU Memory Used: 3.164 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  66%|██████▌   | 66/100 [00:25<00:13,  2.45it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 66: GPU Memory Used: 3.145 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  67%|██████▋   | 67/100 [00:26<00:12,  2.70it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 67: GPU Memory Used: 3.146 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  68%|██████▊   | 68/100 [00:26<00:11,  2.90it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 68: GPU Memory Used: 3.146 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  69%|██████▉   | 69/100 [00:26<00:10,  3.05it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 69: GPU Memory Used: 3.145 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  70%|███████   | 70/100 [00:26<00:09,  3.22it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 70: GPU Memory Used: 3.143 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  71%|███████   | 71/100 [00:27<00:08,  3.37it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 71: GPU Memory Used: 3.143 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  72%|███████▏  | 72/100 [00:27<00:08,  3.40it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 72: GPU Memory Used: 3.146 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  73%|███████▎  | 73/100 [00:27<00:07,  3.42it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 73: GPU Memory Used: 3.148 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  74%|███████▍  | 74/100 [00:28<00:07,  3.44it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 74: GPU Memory Used: 3.146 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  75%|███████▌  | 75/100 [00:28<00:07,  3.46it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 75: GPU Memory Used: 3.145 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  76%|███████▌  | 76/100 [00:28<00:06,  3.47it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 76: GPU Memory Used: 3.145 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  77%|███████▋  | 77/100 [00:28<00:06,  3.54it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 77: GPU Memory Used: 3.143 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  78%|███████▊  | 78/100 [00:29<00:06,  3.54it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 78: GPU Memory Used: 3.145 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  79%|███████▉  | 79/100 [00:29<00:05,  3.54it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 79: GPU Memory Used: 3.145 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  80%|████████  | 80/100 [00:30<00:11,  1.78it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 80: GPU Memory Used: 3.162 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  81%|████████  | 81/100 [00:30<00:09,  2.08it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 81: GPU Memory Used: 3.145 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  82%|████████▏ | 82/100 [00:31<00:07,  2.36it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 82: GPU Memory Used: 3.148 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  83%|████████▎ | 83/100 [00:31<00:06,  2.65it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 83: GPU Memory Used: 3.145 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  84%|████████▍ | 84/100 [00:31<00:05,  2.86it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 84: GPU Memory Used: 3.145 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  85%|████████▌ | 85/100 [00:32<00:04,  3.03it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 85: GPU Memory Used: 3.145 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  86%|████████▌ | 86/100 [00:32<00:04,  3.15it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 86: GPU Memory Used: 3.146 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  87%|████████▋ | 87/100 [00:32<00:03,  3.30it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 87: GPU Memory Used: 3.145 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  88%|████████▊ | 88/100 [00:33<00:05,  2.28it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 88: GPU Memory Used: 3.164 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  89%|████████▉ | 89/100 [00:33<00:04,  2.57it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 89: GPU Memory Used: 3.145 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  90%|█████████ | 90/100 [00:33<00:03,  2.79it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 90: GPU Memory Used: 3.145 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  91%|█████████ | 91/100 [00:34<00:04,  2.04it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 91: GPU Memory Used: 3.145 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  92%|█████████▏| 92/100 [00:36<00:05,  1.39it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 92: GPU Memory Used: 3.166 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  93%|█████████▎| 93/100 [00:36<00:04,  1.69it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 93: GPU Memory Used: 3.146 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  94%|█████████▍| 94/100 [00:37<00:03,  1.58it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 94: GPU Memory Used: 3.164 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  95%|█████████▌| 95/100 [00:38<00:04,  1.23it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 95: GPU Memory Used: 3.162 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  96%|█████████▌| 96/100 [00:38<00:02,  1.53it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 96: GPU Memory Used: 3.145 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  97%|█████████▋| 97/100 [00:39<00:02,  1.22it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 97: GPU Memory Used: 3.166 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  98%|█████████▊| 98/100 [00:40<00:01,  1.51it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 98: GPU Memory Used: 3.150 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  99%|█████████▉| 99/100 [00:41<00:00,  1.20it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Sample 99: GPU Memory Used: 3.166 GB\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 100/100 [00:41<00:00,  2.40it/s]","output_type":"stream"},{"name":"stdout","text":"Sample 100: GPU Memory Used: 3.148 GB\nAvg latency: 0.194 sec\nTokens per sec: 15.20\nAvg perplexity: 30.51\nBLEU Score: 0.000\nROUGE-1 Score: 0.003\nROUGE-L Score: 0.003\nMemory usage (GB): 3.148\nFLOP Reduction (%): 0.00\nRetrieval Latency (sec): 0.000\nF1 Score: 0.000\nKnowledge Retention: 0.000\nMemory Reduction (%): 0.00\nQuery Processing Time (sec): 0.194\nAccuracy Drop: 0.000\nCompression Ratio: 1.00\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":2}]}